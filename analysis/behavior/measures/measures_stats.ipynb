{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import Counter\n",
    "import math\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import scipy\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/anyaji/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize as tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../full.json') as f:\n",
    "    sparse = json.load(f)\n",
    "with open('../dense.json') as f:\n",
    "    dense = json.load(f)\n",
    "with open('../dense10.json') as f:\n",
    "    sparse_74 = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# segmentation agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cstheory.stackexchange.com/questions/6569/edit-distance-between-two-partitions/6582#6582\n",
    "'''\n",
    "d: {1:str, 2:str, ..., 7:str}\n",
    "returns s: e.g. [[1,2,5],[4],[3,6,7]]\n",
    "'''\n",
    "def make_sets(d):\n",
    "    s=[]\n",
    "    rev=defaultdict(list)\n",
    "    for k,v in d.items():\n",
    "        rev[v].append(k)\n",
    "    for ann, ind_set in rev.items():\n",
    "        s.append(ind_set)\n",
    "    return s\n",
    "\n",
    "def weight(s1,s2):\n",
    "    return len(set(s1)&set(s2)) #weight is the # of pieces matching\n",
    "\n",
    "'''\n",
    "l1,l2: e.g. [[1,2,5],[4],[3,6,7]]\n",
    "returns: len(l1)*len(l2) cost matrix matching elm from l1 to l2\n",
    "'''\n",
    "def cost_matrix(l1,l2):\n",
    "    mat = np.zeros((len(l1),len(l2)))\n",
    "    for i in range(len(l1)):\n",
    "        for j in range(len(l2)):\n",
    "            mat[i][j] = weight(l1[i],l2[j])\n",
    "    return mat\n",
    "\n",
    "'''\n",
    "d1,d2: piece-annotation dictionaries\n",
    "returns: number, higher value/cost == higher agreement (MAX number of pieces that do not change)\n",
    "'''\n",
    "def seg_agreement(d1,d2):\n",
    "    cost = cost_matrix(make_sets(d1),make_sets(d2))\n",
    "    row_ind, col_ind = linear_sum_assignment(cost, True) # maximum weight matching\n",
    "    return cost[row_ind, col_ind].sum()\n",
    "            \n",
    "# print(seg_agreement({1:'body',2:'body',3:'body',4:'body',5:'face',6:'side fin',7:'tail fin of whale'},{1:'road',2:'crosswalk',3:'crosswalk',4:'crosswalk',5:'grass',6:'grass',7:'road'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse\n",
    "def stats_SA(data):\n",
    "    file_to_segagr = defaultdict(float)\n",
    "    for file, anns in data.items():\n",
    "        piece_anns = [detail['piece'] for detail in anns]\n",
    "        mean_agr=0\n",
    "        l=len(piece_anns)\n",
    "        for i in range(l-1):\n",
    "            for j in range(i+1,l):\n",
    "                mean_agr += seg_agreement(piece_anns[i],piece_anns[j])\n",
    "        mean_agr /= l*(l-1)/2\n",
    "        file_to_segagr[file] = mean_agr\n",
    "    print(len(file_to_segagr))\n",
    "    return np.mean(np.array(list(file_to_segagr.values()))), np.std(np.array(list(file_to_segagr.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.30105981070548, 0.6178078374155623)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_SA(sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.092532628701481, 0.5311708922677416)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_SA(dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.338957138957139, 0.7690646011005382)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_SA(sparse_74)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# shape / part ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    words = tokenize(x.lower())\n",
    "    wl = [stemmer.stem(w) for w in words if w not in stop_words and (w.islower() or w.isalnum())]\n",
    "    return wl\n",
    "\n",
    "def naming_div(anns, is_whole_anns):\n",
    "    cleaned_ann_list = []\n",
    "\n",
    "    for ann in anns:\n",
    "        if is_whole_anns:\n",
    "            cleaned_ann = clean(ann['whole'])\n",
    "            cleaned_ann_list.append(cleaned_ann)\n",
    "#             print(ann['whole'])\n",
    "        else:\n",
    "            cleaned_ann = []\n",
    "            parts_set = set(ann['piece'].values()) # parts, excluding duplicates\n",
    "            for word in parts_set:\n",
    "                wl = clean(word)\n",
    "                cleaned_ann+= wl\n",
    "            cleaned_ann_list.append(cleaned_ann)\n",
    "#             print(list(set(ann['piece'].values())))\n",
    "\n",
    "    nd = 0\n",
    "    num_ann = len(cleaned_ann_list)\n",
    "    # each annotation\n",
    "    for i in range(len(cleaned_ann_list)):\n",
    "        frq = 0\n",
    "        # each word in one annotation\n",
    "        wl = cleaned_ann_list[i]\n",
    "        for w in wl:\n",
    "            appeared=0\n",
    "            for j in range(len(cleaned_ann_list)):\n",
    "                if j!=i:\n",
    "                    wll = cleaned_ann_list[j]\n",
    "                    if w in wll:\n",
    "                        appeared+=1\n",
    "            frq += 1 - appeared / (num_ann-1) #proportion of the word appearing in other annotations\n",
    "        if len(wl) != 0:\n",
    "            nd += frq/len(wl) # nd += mean frq (1-p) of each annotation\n",
    "    return nd/num_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_ND(data, is_whole):\n",
    "    file_to_nd = {} # {(divergence, unique set), ...}\n",
    "    for file, anns in data.items():\n",
    "        file_to_nd[file] = naming_div(anns, is_whole)\n",
    "    print(len(file_to_nd))\n",
    "    return np.mean(np.array(list(file_to_nd.values()))), np.std(np.array(list(file_to_nd.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shape ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9083859841856889, 0.10913124904487614)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_ND(sparse, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9254725233684035, 0.05837862268068055)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_ND(dense, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8954085579085579, 0.14878097698694126)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_ND(sparse_74, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7647113108587915, 0.18553479097236833)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_ND(sparse, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7900945754417004, 0.14607280818213772)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_ND(dense, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7270269866519867, 0.19591484995477443)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats_ND(sparse_74, False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
