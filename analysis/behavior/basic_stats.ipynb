{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import Counter\n",
    "import math\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import scipy\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/anyaji/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize as tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../dense_final3.json')\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(x):\n",
    "    words = tokenize(x.lower())\n",
    "    wl = [stemmer.stem(w) for w in words if w not in stop_words and (w.islower() or w.isalnum())]\n",
    "    return wl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(a):\n",
    "    print(np.mean(np.array(a)), np.std(np.array(a)))\n",
    "def desc_len(a):\n",
    "    return len(re.findall(r'\\w+', a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2799910474485228 1.6192799516580405\n",
      "13404\n"
     ]
    }
   ],
   "source": [
    "#Length - whole\n",
    "wholeanns_lengths=[]\n",
    "\n",
    "for worker, detail in data['users'].items():\n",
    "    for k, v in detail.items():\n",
    "        if k.startswith('page'):\n",
    "            text=v['whole-annotation']['wholeAnnotation']\n",
    "            wholeanns_lengths.append(desc_len(text))\n",
    "\n",
    "print_stats(wholeanns_lengths)\n",
    "print(len(wholeanns_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3088673698061226 0.7734742484068301\n"
     ]
    }
   ],
   "source": [
    "#Length - part\n",
    "partanns_lengths=[]\n",
    "for worker, detail in data['users'].items():\n",
    "    for k, v in detail.items():\n",
    "        if k.startswith('page'):\n",
    "            parts_set = set(v['piece-annotation'].values()) # parts, excluding duplicates\n",
    "            for part_ann in parts_set:\n",
    "                partanns_lengths.append(desc_len(part_ann))\n",
    "                \n",
    "print_stats(partanns_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.628692927484333 1.2844188468346154\n"
     ]
    }
   ],
   "source": [
    "#Parts per shape\n",
    "nums_parts=[]\n",
    "for worker, detail in data['users'].items():\n",
    "    for k, v in detail.items():\n",
    "        if k.startswith('page'):\n",
    "            c = len(set(v['piece-annotation'].values()))\n",
    "            nums_parts.append(c)\n",
    "print_stats(nums_parts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9290692654043051 1.2027307156975353\n"
     ]
    }
   ],
   "source": [
    "#Pieces per part\n",
    "def count_ppp(d):\n",
    "    '''returns ppp for all parts in this tangram'''\n",
    "    partdesc_to_count=defaultdict(int)\n",
    "    for idx, ann in d.items():\n",
    "        partdesc_to_count[ann]+=1\n",
    "    return list(partdesc_to_count.values())\n",
    "\n",
    "ppp=[]\n",
    "for worker, detail in data['users'].items():\n",
    "    for k, v in detail.items():\n",
    "        if k.startswith('page'):\n",
    "            ppp+=count_ppp(v['piece-annotation'])\n",
    "print_stats(ppp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_vocab(x):\n",
    "    words = tokenize(x.lower())\n",
    "    wl = [stemmer.stem(w) for w in words if (w.islower() or w.isalnum())] \n",
    "    return wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3031\n"
     ]
    }
   ],
   "source": [
    "#Whole vocab\n",
    "whole_vocab=set()\n",
    "for worker, detail in data['users'].items():\n",
    "    for k, v in detail.items():\n",
    "        if k.startswith('page'):\n",
    "            text=clean_vocab(v['whole-annotation']['wholeAnnotation'])\n",
    "            for w in text:\n",
    "                whole_vocab.add(w)\n",
    "print(len(whole_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3110\n"
     ]
    }
   ],
   "source": [
    "#Part vocab\n",
    "part_vocab=set()\n",
    "for worker, detail in data['users'].items():\n",
    "    for k, v in detail.items():\n",
    "        if k.startswith('page'):\n",
    "            piece_anns = set(v['piece-annotation'].values())\n",
    "            for ann in piece_anns:\n",
    "                text=clean_vocab(ann)\n",
    "                for w in text:\n",
    "                    part_vocab.add(w)\n",
    "print(len(part_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4522"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set.union(whole_vocab, part_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
